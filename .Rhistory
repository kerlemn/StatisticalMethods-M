correct_predictions <- sum(predicted_labels == y.new)
print(paste("Total Predictions for observations", length(y.new)))
print(paste("Correct Predictions:", correct_predictions))
# Calculate accuracy
accuracy <- correct_predictions / length(y.new)
print(paste("Accuracy:", accuracy))
# Dummy accuracy
dummy_accuracy <- sum(y.new==0)/length(y.new)
print(paste("Dummy Accuracy:", dummy_accuracy))
# Calculate False Negatives and True Positives
true_positives <- sum(predicted_labels == 1 & y.new == 1)
false_negatives <- sum(predicted_labels == 0 & y.new == 1)
# Calculate True Positive Rate
true_positive_rate <- true_positives / (true_positives + false_negatives)
print(paste("True Positive Rate:", true_positive_rate))
# Calculate AUC
roc_curve <- roc(y.new, predicted_probabilities)
auc_value <- auc(roc_curve)
plot(roc_curve)
print(paste("AUC:", auc_value))
summary(results.logit)
# Split data into train and test sets
set.seed(123)  # Set seed for reproducibility
train_indices <- sample(1:nrow(data), 0.8 * nrow(data))  # 80% train, 20% test
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
Personal.Loan <- which(names(data) == "Personal.Loan")
y<-train_data[,Personal.Loan]
X <- train_data[,-Personal.Loan]
intercept_column <- rep(1, nrow(X))
X <- cbind(intercept_column, X)
X.new <- test_data[,-Personal.Loan]
intercept_column <- rep(1, nrow(X.new))
X.new <- cbind(intercept_column, X.new)
y.new <- test_data[,Personal.Loan]
# Fit Bayesian logistic regression model
results.logit <- UPG(y = y, X = X, model = "logit")
summary(results.logit)
# This suggests that there is credible evidence that the parameter is not equal to 0, based on the Bayesian analysis.
# For example, if the 95% credible interval for a parameter is reported as (0.05, 0.06) with "95% CI excl. 0" notation, it means that there is credible evidence that the parameter is between 0.05 and 0.06, and it is unlikely to be exactly 0.
# Split data into train and test sets
set.seed(123)  # Set seed for reproducibility
train_indices <- sample(1:nrow(data), 0.8 * nrow(data))  # 80% train, 20% test
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
Personal.Loan <- which(names(data) == "Personal.Loan")
y<-train_data[,Personal.Loan]
X <- train_data[,-Personal.Loan]
intercept_column <- rep(1, nrow(X))
X <- cbind(intercept_column, X)
X.new <- test_data[,-Personal.Loan]
intercept_column <- rep(1, nrow(X.new))
X.new <- cbind(intercept_column, X.new)
y.new <- test_data[,Personal.Loan]
# Fit Bayesian logistic regression model
results.logit <- UPG(y = y, X = X, model = "logit")
summary(results.logit)
# This suggests that there is credible evidence that the parameter is not equal to 0, based on the Bayesian analysis.
# For example, if the 95% credible interval for a parameter is reported as (0.05, 0.06) with "95% CI excl. 0" notation, it means that there is credible evidence that the parameter is between 0.05 and 0.06, and it is unlikely to be exactly 0.
print(results.logit)
summary(results.logit)
# Split data into train and test sets
set.seed(123)  # Set seed for reproducibility
train_indices <- sample(1:nrow(data), 0.8 * nrow(data))  # 80% train, 20% test
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
Personal.Loan <- which(names(data) == "Personal.Loan")
y<-train_data[,Personal.Loan]
X <- train_data[,-Personal.Loan]
intercept_column <- rep(1, nrow(X))
X <- cbind(intercept_column, X)
X.new <- test_data[,-Personal.Loan]
intercept_column <- rep(1, nrow(X.new))
X.new <- cbind(intercept_column, X.new)
y.new <- test_data[,Personal.Loan]
# Fit Bayesian logistic regression model
results.logit <- UPG(y = y, X = X, model = "logit")
# This suggests that there is credible evidence that the parameter is not equal to 0, based on the Bayesian analysis.
# For example, if the 95% credible interval for a parameter is reported as (0.05, 0.06) with "95% CI excl. 0" notation, it means that there is credible evidence that the parameter is between 0.05 and 0.06, and it is unlikely to be exactly 0.
summary(results.logit)
library(loo)
# Assuming model_fit is your fitted Bayesian model object
waic_result <- loo(model_fit)
library(loo)
# Assuming model_fit is your fitted Bayesian model object
waic_result <- loo(results.logit)
install.packages("loo")
# Define the step size for updating the model
step_size <- 500
first_step <- 1500
# Initialize a vector to store accuracy for each iteration
accuracy_vector <- numeric()
auc_vector <- numeric()
for (k in seq(from = first_step, to = 5000 - step_size, by = step_size)) {
# Update the model with new data
model_adjusted <- update(
model,
newdata = bayes_data[k :(k+step_size), ],
refresh = 0  # Set refresh to 0 to avoid recomputing warmup samples
)
# Extracting the posterior predictions for the adjusted model
posterior_predictions_adjusted <- posterior_predict(model_adjusted)
# Extracting the response variable for the test data
y_test <- bayes_data[(k + 1):5000, ]$Personal.Loan
# Make predictions on the test data
posterior_predictions_test <- posterior_predict(model_adjusted, newdata = bayes_data[(k + 1):5000, ])
# Convert predictions to probabilities
predicted_probabilities <- apply(posterior_predictions_test, 2, mean) #worse with median
# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.8
predicted_labels <- ifelse(predicted_probabilities > threshold, 1, 0)
correct_predictions <- sum(predicted_labels == y_test)
print(paste("Total Predictions for observations", (k + 1), "to 5000:", length(y_test)))
print(paste("Correct Predictions:", correct_predictions))
# Calculate accuracy
accuracy <- correct_predictions / length(y_test)
print(paste("Accuracy:", accuracy))
# Calculate true positive rate
true_positive_rate <- true_positives / (true_positives + false_negatives)
print(paste("True Positive Rate:", true_positive_rate))
# Calculate AUC
roc_curve <- roc(y_test, predicted_probabilities)
auc_value <- auc(roc_curve)
# Print or store the AUC for this iteration
print(paste("AUC:", auc_value))
# Store AUC in the vector
auc_vector <- c(auc_vector, auc_value)
# Dummy accuracy
dummy_accuracy <- sum(y_test==0)/length(y_test)
print(paste("Dummy Accuracy:", dummy_accuracy))
waic_result <- loo(model_fit)
# Extract WAIC value
waic_value <- waic_result$estimates[1]
# Print WAIC value
print(paste("WAIC:", waic_value))
print(summary(model_adjusted))
full_model = glm(Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard, data = data[1:(k+step_size),], family = binomial(link = logit))
print(full_model)
predictions <- predict(full_model, newdata = data, type = "response")
# 'predictions' will contain the predicted probabilities
# If you want to convert probabilities to classes, you can define a threshold
# For example, if threshold = 0.5, predicted class is 1 if probability >= 0.5, else 0
predicted_classes <- ifelse(predictions >= threshold, 1, 0)
# If 'data' already contains the response variable, you can compare predictions with actual values
actual <- data$Personal.Loan
accuracy <- mean(predicted_classes == actual)
print(paste("Accuracy Logistic model:", accuracy))
}
# First 1000 observations for training
train_data <- bayes_data[1000:1500, ]
bayes_data <- data[c("Personal.Loan", "CD.Account", "Income", "Family", "Education", "CreditCard")]
bayes_data$Personal.Loan <- as.numeric(as.character(bayes_data$Personal.Loan))
# First 1000 observations for training
train_data <- bayes_data[1000:1500, ]
# Fit the Bayesian logistic regression model with the specified prior
model <- brm(
formula = Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard,
data = train_data,
family = bernoulli(link = "logit"),
prior = prior_spec,
chains = 4,
iter = 2000,
warmup = 1000
)
# Install and load required libraries
library(brms)
library(pROC)
full_model = glm(Personal.Loan ~ ., data = data[1:1000,], family = binomial(link = logit))
summary(full_model)
full_model = glm(Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard, data = data[1:1000,], family = binomial(link = logit))
summary(full_model)
bayes_data <- data[c("Personal.Loan", "CD.Account", "Income", "Family", "Education", "CreditCard")]
bayes_data$Personal.Loan <- as.numeric(as.character(bayes_data$Personal.Loan))
# First 1000 observations for training
train_data <- bayes_data[1000:1500, ]
# Fit the Bayesian logistic regression model with the specified prior
model <- brm(
formula = Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard,
data = train_data,
family = bernoulli(link = "logit"),
prior = prior_spec,
chains = 4,
iter = 2000,
warmup = 1000
)
# First 1000 observations for training
train_data <- bayes_data[1000:1500, ]
# Fit the Bayesian logistic regression model with the specified prior
model <- brm(
formula = Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard,
data = train_data,
family = bernoulli(link = "logit"),
chains = 4,
iter = 2000,
warmup = 1000
)
# Display model summary
summary(model)
# Define the step size for updating the model
step_size <- 500
first_step <- 1500
# Initialize a vector to store accuracy for each iteration
accuracy_vector <- numeric()
auc_vector <- numeric()
for (k in seq(from = first_step, to = 5000 - step_size, by = step_size)) {
# Update the model with new data
model_adjusted <- update(
model,
newdata = bayes_data[k :(k+step_size), ],
refresh = 0  # Set refresh to 0 to avoid recomputing warmup samples
)
# Extracting the posterior predictions for the adjusted model
posterior_predictions_adjusted <- posterior_predict(model_adjusted)
# Extracting the response variable for the test data
y_test <- bayes_data[(k + 1):5000, ]$Personal.Loan
# Make predictions on the test data
posterior_predictions_test <- posterior_predict(model_adjusted, newdata = bayes_data[(k + 1):5000, ])
# Convert predictions to probabilities
predicted_probabilities <- apply(posterior_predictions_test, 2, mean) #worse with median
# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.8
predicted_labels <- ifelse(predicted_probabilities > threshold, 1, 0)
correct_predictions <- sum(predicted_labels == y_test)
print(paste("Total Predictions for observations", (k + 1), "to 5000:", length(y_test)))
print(paste("Correct Predictions:", correct_predictions))
# Calculate accuracy
accuracy <- correct_predictions / length(y_test)
print(paste("Accuracy:", accuracy))
# Calculate true positive rate
true_positive_rate <- true_positives / (true_positives + false_negatives)
print(paste("True Positive Rate:", true_positive_rate))
# Calculate AUC
roc_curve <- roc(y_test, predicted_probabilities)
auc_value <- auc(roc_curve)
# Print or store the AUC for this iteration
print(paste("AUC:", auc_value))
# Store AUC in the vector
auc_vector <- c(auc_vector, auc_value)
# Dummy accuracy
dummy_accuracy <- sum(y_test==0)/length(y_test)
print(paste("Dummy Accuracy:", dummy_accuracy))
waic_result <- loo(model_fit)
# Extract WAIC value
waic_value <- waic_result$estimates[1]
# Print WAIC value
print(paste("WAIC:", waic_value))
print(summary(model_adjusted))
full_model = glm(Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard, data = data[1:(k+step_size),], family = binomial(link = logit))
print(full_model)
predictions <- predict(full_model, newdata = data, type = "response")
# 'predictions' will contain the predicted probabilities
# If you want to convert probabilities to classes, you can define a threshold
# For example, if threshold = 0.5, predicted class is 1 if probability >= 0.5, else 0
predicted_classes <- ifelse(predictions >= threshold, 1, 0)
# If 'data' already contains the response variable, you can compare predictions with actual values
actual <- data$Personal.Loan
accuracy <- mean(predicted_classes == actual)
print(paste("Accuracy Logistic model:", accuracy))
}
# Define the step size for updating the model
step_size <- 500
first_step <- 1500
# Initialize a vector to store accuracy for each iteration
accuracy_vector <- numeric()
auc_vector <- numeric()
for (k in seq(from = first_step, to = 5000 - step_size, by = step_size)) {
# Update the model with new data
model_adjusted <- update(
model,
newdata = bayes_data[k :(k+step_size), ],
refresh = 0  # Set refresh to 0 to avoid recomputing warmup samples
)
# Extracting the posterior predictions for the adjusted model
posterior_predictions_adjusted <- posterior_predict(model_adjusted)
# Extracting the response variable for the test data
y_test <- bayes_data[(k + 1):5000, ]$Personal.Loan
# Make predictions on the test data
posterior_predictions_test <- posterior_predict(model_adjusted, newdata = bayes_data[(k + 1):5000, ])
# Convert predictions to probabilities
predicted_probabilities <- apply(posterior_predictions_test, 2, mean) #worse with median
# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.8
predicted_labels <- ifelse(predicted_probabilities > threshold, 1, 0)
correct_predictions <- sum(predicted_labels == y_test)
print(paste("Total Predictions for observations", (k + 1), "to 5000:", length(y_test)))
print(paste("Correct Predictions:", correct_predictions))
# Calculate accuracy
accuracy <- correct_predictions / length(y_test)
print(paste("Accuracy:", accuracy))
# Calculate true positive rate
true_positive_rate <- true_positives / (true_positives + false_negatives)
print(paste("True Positive Rate:", true_positive_rate))
# Calculate AUC
roc_curve <- roc(y_test, predicted_probabilities)
auc_value <- auc(roc_curve)
# Print or store the AUC for this iteration
print(paste("AUC:", auc_value))
# Store AUC in the vector
auc_vector <- c(auc_vector, auc_value)
# Dummy accuracy
dummy_accuracy <- sum(y_test==0)/length(y_test)
print(paste("Dummy Accuracy:", dummy_accuracy))
waic_result <- loo(model_adjusted)
# Extract WAIC value
waic_value <- waic_result$estimates[1]
# Print WAIC value
print(paste("WAIC:", waic_value))
print(summary(model_adjusted))
full_model = glm(Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard, data = data[1:(k+step_size),], family = binomial(link = logit))
print(full_model)
predictions <- predict(full_model, newdata = data, type = "response")
# 'predictions' will contain the predicted probabilities
# If you want to convert probabilities to classes, you can define a threshold
# For example, if threshold = 0.5, predicted class is 1 if probability >= 0.5, else 0
predicted_classes <- ifelse(predictions >= threshold, 1, 0)
# If 'data' already contains the response variable, you can compare predictions with actual values
actual <- data$Personal.Loan
accuracy <- mean(predicted_classes == actual)
print(paste("Accuracy Logistic model:", accuracy))
}
# Define the step size for updating the model
step_size <- 500
first_step <- 1500
# Initialize a vector to store accuracy for each iteration
accuracy_vector <- numeric()
auc_vector <- numeric()
for (k in seq(from = first_step, to = 5000 - step_size, by = step_size)) {
# Update the model with new data
model_adjusted <- update(
model,
newdata = bayes_data[k :(k+step_size), ],
refresh = 0  # Set refresh to 0 to avoid recomputing warmup samples
)
# Extracting the posterior predictions for the adjusted model
posterior_predictions_adjusted <- posterior_predict(model_adjusted)
# Extracting the response variable for the test data
y_test <- bayes_data[(k + 1):5000, ]$Personal.Loan
# Make predictions on the test data
posterior_predictions_test <- posterior_predict(model_adjusted, newdata = bayes_data[(k + 1):5000, ])
# Convert predictions to probabilities
predicted_probabilities <- apply(posterior_predictions_test, 2, mean) #worse with median
# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.8
predicted_labels <- ifelse(predicted_probabilities > threshold, 1, 0)
correct_predictions <- sum(predicted_labels == y_test)
print(paste("Total Predictions for observations", (k + 1), "to 5000:", length(y_test)))
print(paste("Correct Predictions:", correct_predictions))
# Calculate accuracy
accuracy <- correct_predictions / length(y_test)
print(paste("Accuracy:", accuracy))
# Calculate False Negatives and True Positives
true_positives <- sum(predicted_labels == 1 & y.new == 1)
false_negatives <- sum(predicted_labels == 0 & y.new == 1)
# Calculate True Positive Rate
true_positive_rate <- true_positives / (true_positives + false_negatives)
print(paste("True Positive Rate:", true_positive_rate))
# Calculate AUC
roc_curve <- roc(y_test, predicted_probabilities)
auc_value <- auc(roc_curve)
# Print or store the AUC for this iteration
print(paste("AUC:", auc_value))
# Store AUC in the vector
auc_vector <- c(auc_vector, auc_value)
# Dummy accuracy
dummy_accuracy <- sum(y_test==0)/length(y_test)
print(paste("Dummy Accuracy:", dummy_accuracy))
waic_result <- loo(model_adjusted)
# Extract WAIC value
waic_value <- waic_result$estimates[1]
# Print WAIC value
print(paste("WAIC:", waic_value))
print(summary(model_adjusted))
full_model = glm(Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard, data = data[1:(k+step_size),], family = binomial(link = logit))
print(full_model)
predictions <- predict(full_model, newdata = data, type = "response")
# 'predictions' will contain the predicted probabilities
# If you want to convert probabilities to classes, you can define a threshold
# For example, if threshold = 0.5, predicted class is 1 if probability >= 0.5, else 0
predicted_classes <- ifelse(predictions >= threshold, 1, 0)
# If 'data' already contains the response variable, you can compare predictions with actual values
actual <- data$Personal.Loan
accuracy <- mean(predicted_classes == actual)
print(paste("Accuracy Logistic model:", accuracy))
}
# Define the step size for updating the model
step_size <- 500
first_step <- 1500
# Initialize a vector to store accuracy for each iteration
accuracy_vector <- numeric()
auc_vector <- numeric()
for (k in seq(from = first_step, to = 5000 - step_size, by = step_size)) {
# Update the model with new data
model_adjusted <- update(
model,
newdata = bayes_data[k :(k+step_size), ],
refresh = 0  # Set refresh to 0 to avoid recomputing warmup samples
)
# Extracting the posterior predictions for the adjusted model
posterior_predictions_adjusted <- posterior_predict(model_adjusted)
# Extracting the response variable for the test data
y_test <- bayes_data[(k + 1):5000, ]$Personal.Loan
# Make predictions on the test data
posterior_predictions_test <- posterior_predict(model_adjusted, newdata = bayes_data[(k + 1):5000, ])
# Convert predictions to probabilities
predicted_probabilities <- apply(posterior_predictions_test, 2, mean) #worse with median
# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.2
predicted_labels <- ifelse(predicted_probabilities > threshold, 1, 0)
correct_predictions <- sum(predicted_labels == y_test)
print(paste("Total Predictions for observations", (k + 1), "to 5000:", length(y_test)))
print(paste("Correct Predictions:", correct_predictions))
# Calculate accuracy
accuracy <- correct_predictions / length(y_test)
print(paste("Accuracy:", accuracy))
# Calculate False Negatives and True Positives
true_positives <- sum(predicted_labels == 1 & y.new == 1)
false_negatives <- sum(predicted_labels == 0 & y.new == 1)
# Calculate True Positive Rate
true_positive_rate <- true_positives / (true_positives + false_negatives)
print(paste("True Positive Rate:", true_positive_rate))
# Calculate AUC
roc_curve <- roc(y_test, predicted_probabilities)
auc_value <- auc(roc_curve)
# Print or store the AUC for this iteration
print(paste("AUC:", auc_value))
# Store AUC in the vector
auc_vector <- c(auc_vector, auc_value)
# Dummy accuracy
dummy_accuracy <- sum(y_test==0)/length(y_test)
print(paste("Dummy Accuracy:", dummy_accuracy))
waic_result <- loo(model_adjusted)
# Extract WAIC value
waic_value <- waic_result$estimates[1]
# Print WAIC value
print(paste("WAIC:", waic_value))
print(summary(model_adjusted))
full_model = glm(Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard, data = data[1:(k+step_size),], family = binomial(link = logit))
print(full_model)
predictions <- predict(full_model, newdata = data, type = "response")
# 'predictions' will contain the predicted probabilities
# If you want to convert probabilities to classes, you can define a threshold
# For example, if threshold = 0.5, predicted class is 1 if probability >= 0.5, else 0
predicted_classes <- ifelse(predictions >= threshold, 1, 0)
# If 'data' already contains the response variable, you can compare predictions with actual values
actual <- data$Personal.Loan
accuracy <- mean(predicted_classes == actual)
print(paste("Accuracy Logistic model:", accuracy))
}
# Define the step size for updating the model
step_size <- 500
first_step <- 1500
# Initialize a vector to store accuracy for each iteration
accuracy_vector <- numeric()
auc_vector <- numeric()
for (k in seq(from = first_step, to = 5000 - step_size, by = step_size)) {
# Update the model with new data
model_adjusted <- update(
model,
newdata = bayes_data[k :(k+step_size), ],
refresh = 0  # Set refresh to 0 to avoid recomputing warmup samples
)
# Extracting the posterior predictions for the adjusted model
posterior_predictions_adjusted <- posterior_predict(model_adjusted)
# Extracting the response variable for the test data
y_test <- bayes_data[(k + 1):5000, ]$Personal.Loan
# Make predictions on the test data
posterior_predictions_test <- posterior_predict(model_adjusted, newdata = bayes_data[(k + 1):5000, ])
# Convert predictions to probabilities
predicted_probabilities <- apply(posterior_predictions_test, 2, mean) #worse with median
# Convert probabilities to binary predictions (0 or 1)
threshold <- 0.1
predicted_labels <- ifelse(predicted_probabilities > threshold, 1, 0)
correct_predictions <- sum(predicted_labels == y_test)
print(paste("Total Predictions for observations", (k + 1), "to 5000:", length(y_test)))
print(paste("Correct Predictions:", correct_predictions))
# Calculate accuracy
accuracy <- correct_predictions / length(y_test)
print(paste("Accuracy:", accuracy))
# Calculate False Negatives and True Positives
true_positives <- sum(predicted_labels == 1 & y.new == 1)
false_negatives <- sum(predicted_labels == 0 & y.new == 1)
# Calculate True Positive Rate
true_positive_rate <- true_positives / (true_positives + false_negatives)
print(paste("True Positive Rate:", true_positive_rate))
# Calculate AUC
roc_curve <- roc(y_test, predicted_probabilities)
auc_value <- auc(roc_curve)
# Print or store the AUC for this iteration
print(paste("AUC:", auc_value))
# Store AUC in the vector
auc_vector <- c(auc_vector, auc_value)
# Dummy accuracy
dummy_accuracy <- sum(y_test==0)/length(y_test)
print(paste("Dummy Accuracy:", dummy_accuracy))
waic_result <- loo(model_adjusted)
# Extract WAIC value
waic_value <- waic_result$estimates[1]
# Print WAIC value
print(paste("WAIC:", waic_value))
print(summary(model_adjusted))
full_model = glm(Personal.Loan ~ CD.Account + Income + Family + Education + CreditCard, data = data[1:(k+step_size),], family = binomial(link = logit))
print(full_model)
predictions <- predict(full_model, newdata = data, type = "response")
# 'predictions' will contain the predicted probabilities
# If you want to convert probabilities to classes, you can define a threshold
# For example, if threshold = 0.5, predicted class is 1 if probability >= 0.5, else 0
predicted_classes <- ifelse(predictions >= threshold, 1, 0)
# If 'data' already contains the response variable, you can compare predictions with actual values
actual <- data$Personal.Loan
accuracy <- mean(predicted_classes == actual)
print(paste("Accuracy Logistic model:", accuracy))
}
